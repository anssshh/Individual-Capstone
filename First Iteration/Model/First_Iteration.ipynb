{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFKWHG0g4TBj",
        "outputId": "09deb859-40c8-4c7e-aa38-d9deb52dbdd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2429 - loss: 2.0271 - val_accuracy: 0.6330 - val_loss: 1.2673\n",
            "Epoch 2/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6728 - loss: 1.0268 - val_accuracy: 0.7615 - val_loss: 0.6376\n",
            "Epoch 3/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8355 - loss: 0.5159 - val_accuracy: 0.8624 - val_loss: 0.4396\n",
            "Epoch 4/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 0.3761 - val_accuracy: 0.8440 - val_loss: 0.3898\n",
            "Epoch 5/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8760 - loss: 0.3173 - val_accuracy: 0.8257 - val_loss: 0.3848\n",
            "Epoch 6/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8896 - loss: 0.2857 - val_accuracy: 0.8073 - val_loss: 0.3962\n",
            "Epoch 7/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8980 - loss: 0.2647 - val_accuracy: 0.8073 - val_loss: 0.4128\n",
            "Epoch 8/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8978 - loss: 0.2514 - val_accuracy: 0.7982 - val_loss: 0.4301\n",
            "Epoch 9/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9021 - loss: 0.2402 - val_accuracy: 0.7982 - val_loss: 0.4393\n",
            "Epoch 10/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9047 - loss: 0.2276 - val_accuracy: 0.7982 - val_loss: 0.4506\n",
            "Epoch 11/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9210 - loss: 0.2157 - val_accuracy: 0.7982 - val_loss: 0.4460\n",
            "Epoch 12/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9220 - loss: 0.2063 - val_accuracy: 0.7798 - val_loss: 0.4547\n",
            "Epoch 13/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9253 - loss: 0.1946 - val_accuracy: 0.7890 - val_loss: 0.4625\n",
            "Epoch 14/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9374 - loss: 0.1828 - val_accuracy: 0.7890 - val_loss: 0.4882\n",
            "Epoch 15/15\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9374 - loss: 0.1736 - val_accuracy: 0.7890 - val_loss: 0.4932\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8104 - loss: 0.4457 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7890\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# 1. Load data\n",
        "df = pd.read_csv('df_cleaned.csv')  # Sesuaikan nama file\n",
        "X = df['Text']  # Ganti dengan nama kolom teks Anda\n",
        "y = df.drop(columns='Text').idxmax(axis=1)\n",
        "\n",
        "# 2. Encode target labels\n",
        "label_mapping = {label: idx for idx, label in enumerate(y.unique())}\n",
        "y = y.map(label_mapping)\n",
        "\n",
        "# 3. Tokenization and padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Padding sequences\n",
        "max_length = max(len(seq) for seq in X_seq)\n",
        "X_padded = pad_sequences(X_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# 4. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_padded, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# 5. Convert target labels to one-hot encoding\n",
        "y_train_encoded = to_categorical(y_train, num_classes=len(label_mapping))\n",
        "y_test_encoded = to_categorical(y_test, num_classes=len(label_mapping))\n",
        "\n",
        "# 6. Build model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(label_mapping), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 7. Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train_encoded,\n",
        "    epochs=15,\n",
        "    batch_size=4,\n",
        "    validation_data=(X_test, y_test_encoded)\n",
        ")\n",
        "\n",
        "# 8. Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 9. Save the model\n",
        "model.save('text_classification_model.h5')\n"
      ]
    }
  ]
}